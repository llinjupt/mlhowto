深度学习笔记
================

基本概念
------------------

- 人工智能 （AI Artificial Intelligence），努力将通常由人类完成的智力任务自动化。
- 机器学习 （ML Machine Learning）
- 深度学习 （DL Deep Learning）

DL 是 ML 算法中的一种。ML 是实现 AI 的方法。

包含关系，AI > ML > DL。

AI
~~~~~~~~~~~~~

人工智能发展阶段：

- AI 诞生于 20 世纪 50 年代，人们提出疑问：计算机是否能够“思考”？
- 符号主义人工智能（Symbolic AI），人们相信，只要编写足够多的“明确规则”（硬编码）就可以实现 AI。专家系统（Expert System）是此种方式的典型应用。
- ML 机器学习，符号主义人工智能适用于解决另一明确的逻辑问题，比如国际象棋，不适用于更加复杂，模糊的问题，比如图像分类，声音识别，于是 ML 取代了它。

ML
~~~~~~~~~~~~~

如果没有程序员精心编写的数据处理规则，计算机能否通过观察数据自动学会这些规则？

- 符号主义人工智能：输入规则和处理数据，系统输出答案。

.. code-block:: sh
  :linenos:
  :lineno-start: 0 
  
  +------+     +-----------------------+     +---------+
  | Data | --> | Classical Programming | --> | Answers |
  +------+     +-----------------------+     +---------+
                 ^
                 |
               +-----------------------+
               |         Rules         |
               +-----------------------+


- ML：人们输入数据（样本）和从这些数据中预期得到的答案，系统输出的是规则。规则可用于新的数据，并使计算机自主生成答案。

.. code-block:: sh
  :linenos:
  :lineno-start: 0 
  
  +---------+     +------------------+     +-------+
  | Answers | --> | Machine Learning | --> | Rules |
  +---------+     +------------------+     +-------+
                    ^
                    |
                  +------------------+
                  |       Data       |
                  +------------------+

机器学习的最大特点：机器学习系统是训练出来的，而不是明确用程序编写出来的。举个例子，给 ML 分析一些猫的照片，机器会学习到猫的特征规则，并可以识别新的照片中的动物是否是猫。

机器学习的蓬勃发展的驱动力来源于硬件速度的提升和更大的数据集。例如：

- Flickr 网站上用户生成的图像标签一直是计算机视觉的数据宝库。 
- YouTube 视频也是一座宝库。
- 维基百科则是自然语言处理的关键数据集。

机器学习与数理统计密切相关，但是不同于统计学，它是一门以工程为导向的科学，更多的靠实践来证明，而不是理论推导。

机器学习三要素：

1. 输入数据集。 比如语音数据，图像数据。
2. 预期输入的示例。对于图像输入来说预期输入可能是“猫狗”之类的标签。
3. 衡量算法效果好坏的方法。为了计算算法的当前输入出与预期输出的差距。

衡量结果是一种反馈信号，用于调节算法的工作方式，这个调节步骤就是我们所说的 **学习**。

机器学习的核心问题是为输入数据寻找合适的表示——对数据进行变换，使其更适合手头的任务（比如分类任务）。
例如识别认证码，我们就不关心色彩，而是要把它与背景区分开来并校正扭曲的码字。

机器学习中的 **学习** 指的是，寻找更好数据表示（根据任务将数据转化为更加有用的表示）的自动搜索过程。

机器学习算法在寻找这些变换时仅仅是遍历一组预先定义好的操作，这组操作叫做 **假设空间** （Hypothesis Space）。

这就是机器学习的技术定义：在预先定义好的可能性空间中，利用反馈型号的指引来寻找输入数据的有用表示。整个流程如下所示：

.. code-block:: sh
  :linenos:
  :lineno-start: 0 
  
  +-----------+     +------------------+     +---------+       +-----------+
  |   Data    | --> | Machine Learning | --> | Answers | <-|-> | Expection |
  +-----------+     +------------------+     +---------+   |   +-----------+
                            ^                              |
                            | Transform                    | Feedback              
                  +------------------+     +-----------+   |                              
                  | Hypothesis Space | <-- |   Diff    | <-+
                  +------------------+     +-----------+

DL
~~~~~~~~

数据模型中包含多少层，这被称为模型的 **深度** （depth）。

这一领域的其他名称包括分层表示学习（layered representations learning）和层级表示学习（hierarchical representations learning）。

- 深度学习通常包含数十个甚至上百个连续的表示层，这些表示层全都是从训练数据中自动学习的。
- 其他机器学习方法的重点往往是仅仅学习一两层的数据表示，因此有时也被称为浅层学习（shallow learning）。

在深度学习中，这些表示层通过神经网络（neural network）的模型来学习得到。神经网络的结构是 **逐层堆叠** 。

深度学习的技术定义：学习数据表示的多级方法。

神经网络中每层对输入数据所做的具体操作保存在该层的 **权重** (weight) 。其本质是一串数字，权重也被称为该层的 **参数** （parameter）。

学习的意思是为神经网络的每层找到一组权重值，使得该网络能够将每个示例输入与其目标正确地一一对应。

神经网络的 **损失函数** （loss function），或目标函数（objective function）用于衡量输出与预期之间的距离，也即效果的好坏。

DL 的基本技巧是利用这个距离值作为反馈信号来对权重值进行微调，以降低损失值，这种调节由优化器（Optimizer）来完成，它实现了所谓的反向传播（backpropagation） 算法。这是 DL 的核心算法。

一开始对权重随机赋值，随着网络处理的示例越来越多，权重值也在向正确方向趋近，损失值也逐渐降低，这就是训练循环（Training Loop）。

深度学习从数据中进行学习时有两个基本特征：

- 第一， 通过渐进的、逐层的方式形成越来越复杂的表示；
- 第二， 对中间这些渐进的表示共同进行学习，每一层的变化都需要同时考虑上下两层的需要。

这两个特征使得深度学习比先前的机器学习方法更加成功。

深度学习已经取得了以下突破，它们都是机器学习历史上非常困难的领域：

- 接近人类水平的图像分类
- 接近人类水平的语音识别
- 接近人类水平的手写文字转录
- 更好的机器翻译
- 更好的文本到语音转换
- 数字助理，比如谷歌即时（Google Now）和亚马逊 Alexa
- 接近人类水平的自动驾驶
- 更好的广告定向投放， Google、百度、必应都在使用
- 更好的网络搜索结果
- 能够回答用自然语言提出的问题
- 在围棋上战胜人类

经典机器学习方法
~~~~~~~~~~~~~~~~~

概率建模
```````````````````

概率建模是统计学原理在数据分析中的应用。它是最早的机器学习形式之一。

- 朴素贝叶斯算法，它假设输入数据的特征都是独立的。
- Logistic 回归（Logistic Regression，简称 Logreg），它是一种分类算法，而不是回归算法。

早期神经网络
``````````````

贝尔实验室于 1989 年第一次成功实现了神经网络的实践应用，当时 Yann LeCun 将卷积神经网络的早期思想与反向传播算法相结合，并将其应用于手写数字分类问题，由此得到名为 LeNet 的网络，在 20 世纪 90 年代被美国邮政署采用，用于自动读取信封上的邮政编码。

核方法
`````````````

核方法（kernel method）。核方法是一组分类算法，其中最有名的就是支持向量机（SVM，support vector machine）。

SVM 的目标是通过在属于两个不同类别的两组数据点之间找到良好决策边界（decision boundary）来解决分类问题。

SVM 通过两步来寻找决策边界。
1. 将数据映射到一个新的高维表示，这时决策边界可以用一个超平面来表示。
2. 尽量让超平面与每个类别最近的数据点之间的距离最大化，从而计算出良好决策边界（分割超平面），这一步叫作间隔最大化（maximizing the argin）。这样决策边界可以很好地推广到训练数据集之外的新样本。

SVM 很难扩展到大型数据集，并且在图像分类等感知问题上的效果也不好。 SVM 是一种比较浅层的方法，因此要想将其应用于感知问题，首先需要手动提取出有用的表示（这叫作特征工程），这一步骤很难，而且不稳定。

决策树、随机森林
```````````````````

决策树（decision tree）是类似于流程图的结构，可以对输入数据点进行分类或根据给定输入来预测输出值。

随机森林（random forest）算法，它引入了一种健壮且实用的决策树学习方法，即首先构建许多决策树，然后将它们的输出集成在一起。随机森林适用于各种各样的问题——
对于任何浅层的机器学习任务来说，它几乎总是第二好的算法。

梯度提升机
```````````````````

与随机森林类似， 梯度提升机（gradient boosting machine）也是将弱预测模型（通常是决策树）集成的机器学习技术。它使用了梯度提升方法，通过迭代地训练新模型来专门解决之前模型的弱点，从而改进任何机器学习模型的效果。

基础数学
------------------

比较全面的总结参考这里：`机器学习中的基本数学知识 <https://www.cnblogs.com/steven-yang/p/6348112.html>`_ 。

张量
~~~~~~~~

张量是矩阵向任意维度的推广［注意，张量的维度（dimension）通常叫作轴（axis）］。矩阵是 2D 张量。

张量轴的个数也叫作阶 （rank）。
维度可以表示沿着某个轴上的元素个数，也可以表示张量中轴的个数。

- 0D 张量：仅包含一个数字的张量叫作标量(scalar)，对应 Numpy 中 一个 float32 或 float64 数字。
- 1D 张量：数字组成的1维数组叫作向量（vector），它有一个轴。如果一个向量有 5 个元素，称为 5D向量。5D 向量只有一个轴，沿着轴有5个维度。
- 2D 张量：向量组成的数组叫作矩阵（matrix）或者二维张量。矩阵有2个轴，通常被叫作行和列。
- 3D 张量：多个矩阵组合成一个新的数组，可以得到一个 3D 张量。

深度学习处理的一般是 0D 到 4D 张量，但处理视频时会遇到 5D 张量。

我们用几个你未来会遇到的示例来具体介绍数据张量。你需要处理的数据几乎总是以下类别之一。

- 向量数据：2D 张量，形状为 (samples, features)。
- 时间序列数据或序列数据： 3D 张量，形状为 (samples, timesteps, features)。
- 图像： 4D 张量，形状为 (samples, height, width, channels) 或 (samples, channels,height, width)。
- 视频： 5D 张量，形状为 (samples, frames, height, width, channels) 或 (samples, frames, channels, height, width)。

概率论
~~~~~~~~

样本和事件
```````````

.. role:: raw-latex(raw)
    :format: latex html

**样本空间**：考虑一个试验，其结果是不可肯定地预测的（是一个具有随机性的变量），则所有可能的结果构成的集合，称为该试验的样本空间（Sample Space），记为 S，所以S 就是随机变量所有的可能值的集合。

例如：试验是考察新生婴儿的性别，那么所有可能结果的集合 S = {girl, boy}。

**事件(event)**：样本空间的任一子集 E 称为事件。一个事件是由试验的部分结果组成的一个集合，如果试验的结果包含在 E 里面，那么就称事件 E 发生了。

例如，E={girl} 就是一个事件，如果考察的出生的婴儿是女孩，那么就表示事件 E （婴儿是个女孩）发生了。

.. math::
  
  P\left( E\right) =\lim _{n\rightarrow \infty }\dfrac {n\left( E\right) }{n}

n(E)表示 n 次重复试验中事件E发生的次数，概率 P(E) 就定义成上面的形式。关于概率的几个简单性质：

- 一个事件不发生的概率等于 1 减去它发生的概率。:raw-latex:`\(P\left( E^{c}\right) =1-P\left( E\right)\)`。
- 如果 :raw-latex:`\(E\subset F\)`，那么 :raw-latex:`\(P\left( E\right) \leq P\left( F\right)\)`。
-  :raw-latex:`\(P\left( E\cup F\right) =P\left( E\right) +P\left( F\right) -P\left( EF\right)\)`。

使用维恩图很容易理解。

信息熵
```````````

信息熵(Information Entropy)用于对样本集D在样本空间（所有可能结果的集合）分布的纯度的度量：分布越单一，那么纯度越高，熵越小，分布越杂乱，那么纯度越底，熵越大。

.. figure:: imgs/park.jpg
  :scale: 50%
  :align: center
  :alt: 信息熵

  公园步道和绿化带

举一个直觉上的例子，把公园中的步道和绿化带做对比，步道的规律非常明显，无论从材料还是从铺设上都是有规律可循的，我们只要观察一小部分就能推出其余步道规律，它所包含的信息量就少，可以类比数据“abcabcabc”；而绿化带就没有那么明显的规律，有草有树，还有落叶和有了设施，地势也高低不平，那么它所包含的信息量就大，可以类比于数据“Hello world！”。

.. math::

  Ent(D)=-\sum_{k=1}^npk\log_{2}^{pk}

以上就是熵的定义，单位为bit。pk 表示每一样本空间的概率，当分类最具规律时，也即全部属于一个分类 i 时，必有 pi = 1，其余分类概率为 0， 此时 Ent(D) 取最小值0，当所有 k 均匀分布时（最杂乱），Ent(D) 取最大值 
:raw-latex:`\(\log_{2}^{n}\)` 。

以2为底的信息熵还直接给出了最小二进制编码长度，信息熵的单位为 bit 是有实际意义的。如何度量一篇英文文章的信息量，以及如何最小编码？英文文章中的每一个字符可以使用128个ASCII码表示，如果每个ASCII码等概率出现，那么每个字符的 Ent(D) 就是 7bits，就需要 7bits 进行编码，然而字符 a 比 z 出现的概率要高得多，所以 a 可以使用更短的编码: （:raw-latex:`\(\log_{2}^{pa}\)` 向上取整），z 可以使用更长的编码，平均下来每个字符的 Ent(D) 就远远少于 7bits。

由于编码后的字节是连续的，要区分不同的字符的编码就要加入控制字符，比如前导码，这就是数据冗余。以上就是香农-范诺编码和哈夫曼编码的理论基础。

决策树分类方法使用最大信息增益（或基尼系数）构造决策树，每一步选择分类的属性标准：让数据更趋于有规律，此时信息增益最大，而整个分类的信息熵就会降低最大。

几种概率
```````````

**先验概率**： :raw-latex:`\(P(X=a)\)` 仅与单个随机变量有关的概率称为先验概率。又称为 **边缘概率** 或 **统计概率**，因为它不考虑其他因素（不限定任何条件）。

例如一个箱子中有 0-9 编号的 10 个球，从中单次取到编号为 0 的球的概率 :raw-latex:`\(P(X=0) = \frac{1}{10}\)` 。

**条件概率**：在 Y = b 成立的情况之下，X = a 的概率，记作 :raw-latex:`\(P(X=a|Y=b)\)` 或 :raw-latex:`\(P(a|b)\)`。是已知 b 发生后发生 a 的条件概率，也称作 a 在给定 b 条件下的 **后验概率**。

例如当从第一个箱子取到编号为 0 的情况下，再从箱子中取到 1 号球的概率：:raw-latex:`\(P(1|0) = \frac{1}{9}\)`

**联合概率**：包含多个条件且所有条件同时成立的概率，记作 :raw-latex:`\(P(X=a,Y=b)\)` 或 :raw-latex:`\(P(a,b)\)`。

例如有两个箱子，一个箱子中有 0-9 编号的10个球，另一个箱子有红绿 2 种颜色的 2 个球。从第一个箱子取到数字 0 并且第二个箱子取到红色球的概率 :raw-latex:`\(P(0,Red) = \frac{1}{20}\)` 。

各类概率间关系
```````````````

**联合概率、边缘概率与条件概率之间的关系**：

.. math::
  
  \ P(X=a|Y=b)=\frac{P(X=a,Y=b)}{P(Y=b)}

简写为：

.. math::

  \ P(X|Y)=\frac{P(X,Y)}{P(Y)}


由于 :raw-latex:`\(P(X=a,Y=b)=P(Y=b,X=a)\)`，所以 ：

.. math::
  
  \ P(Y=b|X=a)=\frac{P(X=a|Y=b)P(Y=b)}{P(X=a)}

简写为：

.. math::
  
  \ P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}  

**独立事件**：如果事件 Y 发生不会影响事件 X 的发生，则称X, Y相互独立。而在条件概率中，当 :raw-latex:`\(P(X|Y)=P(X)\)` 时也就意味着 Y 发生不会影响 X 发生。
代入 :raw-latex:`\(P(X|Y)=\frac{P(X,Y)}{P(Y)}\)` 得到 :raw-latex:`\(P(X,Y)={P(X)}{P(Y)}\)`，它通常用于判断两个事件是否独立。

上面例子中的从两个箱子拿小球的例子就是相互独立，所以  :raw-latex:`\(P(X,Y)={P(X)}{P(Y)}\)`，也即  :raw-latex:`\(\frac{1}{20}=\frac{1}{10}\frac{1}{2}\)`。

**非独立事件**：如果不满足  :raw-latex:`\(P(X,Y)={P(X)}{P(Y)}\)`，那么 X, Y 就是非独立事件。

例如在一个箱子里面有编号 0-9 的 10 个小球，其中奇数球为红色，偶数球为绿色，那么取到 0 号球的事件 X 概率为 :raw-latex:`\(P(X)=\frac{1}{10}\)`，取到红色球的事件 Y 概率为 :raw-latex:`\(P(Y)=\frac{1}{2}\)`，但是同时取到 0 号球和红色球的概率 :raw-latex:`\(P(X,Y)\neq{P(X)}{P(Y)}\)`，而是 0。

无论是独立事件还是非独立事件，下面的等式恒成立：

.. math::
  
  \ \frac{P(Y|X)}{P(Y)}=\frac{P(X|Y)}{P(X)}  

这意味着互为条件的概率比值是一个常数。

举例子：

- 假如发现金矿的地形多在靠近河流的山地，那么如果新发现一个金矿，那么这个新矿的地形很可能是靠近河流的山地。
- 假如统计发现垃圾邮件中多出现 “大奖”，“发票”，“陪聊”，“大乐透”等关键词，那么当一个邮件内出现这些词的时候，它是垃圾邮件的概率就很高。

这就是贝叶斯分类的理论依据。

